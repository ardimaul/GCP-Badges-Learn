# Ensure Access & Identity in Google Cloud: Challenge Lab
```bash
export MY_PROJECT=Your project ID
#example,
export MY_PROJECT=qwiklabs-gcp-02-11881cf6ba00
```

## Task 1: Create a custom security role.
```bash
# Create custom role with its permissions config
echo "title: \"orca_storage_update\"
description: \"Permissions\"
stage: \"ALPHA\"
includedPermissions:
- storage.buckets.get
- storage.objects.get
- storage.objects.list
- storage.objects.update
- storage.objects.create" > custom-role-orca_storage_update.yaml
```
```bash
# Apply config
gcloud iam roles create orca_storage_update --project $MY_PROJECT \
--file custom-role-orca_storage_update.yaml
```

## Task 2: Create a service account.
```bash
# Create service account for cluster
gcloud iam service-accounts create orca-private-cluster-sa --display-name "Orca private cluster SA"
```

## Task 3: Bind a custom security role to a service account.
Bind service account with custom security role that we have created it earlier, its for accessing the bucket
```bash
gcloud alpha projects add-iam-policy-binding $MY_PROJECT --member serviceAccount:orca-private-cluster-sa@$MY_PROJECT.iam.gserviceaccount.com \
--role "projects/$MY_PROJECT/roles/orca_storage_update"
```
Binding logging and monitoring role, its kubernetes cluster feature logging and monitoring and its enabled by default, if you disabled that feature you dont need this role
```bash
gcloud projects add-iam-policy-binding $MY_PROJECT --member serviceAccount:orca-private-cluster-sa@$MY_PROJECT.iam.gserviceaccount.com \
--role "roles/monitoring.viewer"
gcloud projects add-iam-policy-binding $MY_PROJECT --member serviceAccount:orca-private-cluster-sa@$MY_PROJECT.iam.gserviceaccount.com \
--role "roles/logging.logWriter"
gcloud projects add-iam-policy-binding $MY_PROJECT --member serviceAccount:orca-private-cluster-sa@$MY_PROJECT.iam.gserviceaccount.com \
--role "roles/monitoring.metricWriter"
```

## Task 4: Create and configure a new Kubernetes Engine private cluster
Create Private kubernetes cluster, with custom service account, with orca-build-subnet subnetwork so it can be accessed by orca-jumphost vpc
```bash
gcloud container clusters create orca-test-cluster --num-nodes 1 --master-ipv4-cidr=172.16.0.64/28 \
--network orca-build-vpc --subnetwork orca-build-subnet --enable-master-authorized-networks  \
--master-authorized-networks 192.168.10.2/32 --enable-ip-alias --enable-private-nodes \
--enable-private-endpoint --service-account orca-private-cluster-sa@$MY_PROJECT.iam.gserviceaccount.com \
--zone us-east1-b
```

## Task 5: Deploy an application to a private Kubernetes Engine cluster.
Ssh into orca-jumphost vpc
```bash
gcloud compute ssh orca-jumphost --zone us-east1-b
```
Get credentials and make it with internal-ip because orca-jumphost (vpc) and orca-test-cluster (kubernetes cluster) communicate with internal ip (orca-build-subnet)
```bash
gcloud container clusters get-credentials orca-test-cluster --internal-ip --zone us-east1-b
sudo apt update && sudo apt install kubectl -y
# Test to create deployment
kubectl create deployment hello-server --image=gcr.io/google-samples/hello-app:1.0
```
Fun fact, if you have enable private endpoint you will not be able to access them including the Gcloud shell except by adding master authorized networks, so be careful to manage gke in production, cheers
> Created at 09 March 2021, ardimaul17
